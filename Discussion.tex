\subsection{GPU Resource Limitations and Impact on Results}
The limited availability of GPU resources has played a significant role in shaping the outcomes of this study, extending the training duration and inhibiting the development of highly sophisticated models. Furthermore it has limited the amount of datapoints obtained during training to accurately perform this overview. To thoroughly examine the relationship between model complexity and the emergence of double descent, future investigations should allocate substantial GPU resources, enabling in-depth analysis of various model configurations and their performance.

\subsection{Existing Knowledge Gaps and the Ambiguity of Double Descent}
The concept of double descent, first introduced in 2019 (\cite{Belkin2019ReconcilingTrade-off}), remains relatively novel, leading to several knowledge gaps in the field. As a result, some facets of the observed outcomes cannot be explained with absolute certainty, indicating that further research is needed to elucidate the intricacies of the double descent phenomenon. A hypothesis that has been adapted for the cause of double descent is the lottery ticket hypothesis. The hypothesis states that "A randomly-initialized, dense neural network contains a sub-network that is initialized such that — when trained in isolation — it can match the test accuracy of the original network after training for at most the same number of iterations. \cite{Frankle2018TheNetworks}" The paper proposes that large enough neural networks that are randomly initialized, eventually find well performing pathways called winning tickets during training. These tickets were isolated to test their existence via a technique called pruning which is basically removing parameters from the model as training continues. This lottery ticket hypothesis has been well documented and positive in findings for example in the Optimal brain damage (OBD) paper \cite{Denker2014OptimalDamage} and in \cite{Frankle2018TheNetworks}. Relating it back to double descent, A large enough model gives more chance of finding winning tickets. Therefore during training, the model first struggles to find winning tickets (the ascent) but once found, they increase the generalization and performance of the overall model (second descent). The winning tickets are only present during the second descent. Of course to prove this hypothesis, further research has to be conducted perhaps only initializing pruning techniques when the second descent has been observed and documenting its performance on the loss curve.

\subsection{Impact of Hyperparameter Choices on Double Descent}
The choice of hyperparameters, such as learning rate, batch size, and regularization methods, can significantly influence the presence and prominence of the double descent curve. To gain a comprehensive understanding of the methodology, future studies should adopt a critical approach in evaluating optimal hyperparameter configurations for diverse model structures within the context of double descent, considering their potential effects on model performance.

\subsection{Assessing the Generalizability of Findings}
The present study primarily focuses on histopathology image analysis related to metastatic breast cancer, raising questions about the generalizability of the findings to other medical image analyses or different domains altogether. To ascertain the prevalence and applicability of the double descent phenomenon across multiple applications, subsequent research endeavors should extend their scope to include a variety of image analysis tasks and datasets. Double descent is however a general phenomenon in machine learning thus the focus of doing another research to study generalization should focus more on performance or architecture of the model on different tasks such as segmentation or multi-classification.

\subsection{Explanation of the AUC scores and the loss findings}
Models 1 and 3 are outperformed by models 4, 2, 5 and 6 in that order. With 6 being the simplest model trained for this case, these results do not support the claim that a double descent or a more complex model automatically lead to a better performance on the test set. It was decided that because of technical limitations regarding GPU usage and training time, that the most complex model that could be created within reason was one with 4 convolutional layers and 3 dense layers. However, reduction of parameters meant that some dense layers needed to be removed but that only results into 3 datapoints to describe the relation complexity has on the double descent phenomenon. To compensate for that, even more simplification was induced by removing a convolutional layer. However, removing a CN layer removes more weights than a dense layer and has other effects due to the reduction of filters. The only way to compare the models was to compare the one with an extra CN layer (model 1, 2 and 3 as the ones capable of double descent) to the ones who were over simplified to show any sign of that phenomenon (model 4, 5 and 6). One thing in common is that all models with 4 CN layers, have been outperformed by their counterpart which could mean that for this classification task and dataset combination, A lower amount of filters, is actually better suited because increasing the filters used might abstract the data in such a way that it results in increased validation loss and test performance. Still, model 2 represented the best case of double descent and it also obtained the highest AUC score of the 3 that with 4 CN layers. Why model 2 shows a better double descent than model 1 is still unclear but it might be the fact that with every dense layer dropped, a dropout layer was also removed in our experiment. Dropout layers are mechanisms to reduce overfitting of the data. We inserted the dropout layer to introduce more variance in the model and to slightly boost the training time of the model as well. However, using dropout layers does reduce the chance of overfitting which might impact the ability for a loss curve to display a double desent. More dropout layers like in model 1 would therefore hurt the double descent instead of adding complexity. This should be tested by running the models with and without dropout layers and documenting the effect on double descent in further research.

\subsection{Influence of Data Preprocessing and Augmentation on Double Descent}
The impact of data preprocessing and augmentation techniques on the double descent curve constitutes a critical area of inquiry. Preprocessing procedures, such as normalization, resizing, and data augmentation, could potentially affect the model's performance and vulnerability to double descent. An in-depth investigation of the interplay between data preprocessing and model complexity is essential to derive best practices for mitigating the effects of double descent in real-world applications.

\subsection{Exploration of Alternative Deep Learning Architectures}
Finally, to understand the double descent phenomenon comprehensively, it is crucial to examine alternative deep learning architectures, including DenseNets, Inception networks, or Transformer-based models. This analysis will help determine if the double descent phenomenon is more optimal to Convolutional Neural Networks (CNNs) or if it presents a higher occurance and or performance in other deep learning models. By critically examining various architectures, researchers can gain insights into the underlying mechanisms responsible for the double descent phenomenon and identify optimal design choices for a wide range of model architectures, ultimately contributing to the development of more robust and effective solutions across diverse applications.